# ğŸšŒ Desafio CIVITAS - Pipeline BRT Rio de Janeiro

[![Prefect](https://img.shields.io/badge/Prefect-1.4.1-blue)](https://docs.prefect.io/)
[![DBT](https://img.shields.io/badge/dbt-1.5.0-orange)](https://docs.getdbt.com/)
[![BigQuery](https://img.shields.io/badge/BigQuery-GCP-blue)](https://cloud.google.com/bigquery)
[![Python](https://img.shields.io/badge/Python-3.8+-green)](https://www.python.org/)

Pipeline de dados ELT para captura, armazenamento e transformaÃ§Ã£o de dados GPS em tempo real dos veÃ­culos BRT do Rio de Janeiro, seguindo a **Arquitetura Medallion** (Bronze â†’ Silver â†’ Gold).

## ğŸ“‹ Ãndice

- [VisÃ£o Geral](#-visÃ£o-geral)
- [Arquitetura](#-arquitetura)
- [Tecnologias](#-tecnologias)
- [PrÃ©-requisitos](#-prÃ©-requisitos)
- [InstalaÃ§Ã£o](#-instalaÃ§Ã£o)
- [ConfiguraÃ§Ã£o](#-configuraÃ§Ã£o)
- [ExecuÃ§Ã£o](#-execuÃ§Ã£o)
- [Estrutura do Projeto](#-estrutura-do-projeto)
- [Modelos DBT](#-modelos-dbt)
- [Testes de Qualidade](#-testes-de-qualidade)
- [Monitoramento](#-monitoramento)
- [Troubleshooting](#-troubleshooting)

## ğŸ¯ VisÃ£o Geral

Este projeto implementa um pipeline completo de dados que:

1. **Captura** dados GPS dos veÃ­culos BRT minuto a minuto via API
2. **Agrega** 10 minutos de dados em um Ãºnico arquivo CSV
3. **Armazena** no Google Cloud Storage (GCS)
4. **Cria** tabela externa no BigQuery usando DBT
5. **Transforma** dados atravÃ©s de modelos DBT (camadas Silver e Gold)
6. **Documenta** automaticamente tabelas e colunas no BigQuery

### Arquitetura Medallion

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ARQUITETURA MEDALLION                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  ğŸ“¡ API BRT                                                 â”‚
â”‚      â†“                                                      â”‚
â”‚  ğŸ¥‰ BRONZE (Raw Data)                                       â”‚
â”‚      â€¢ Captura minuto a minuto                              â”‚
â”‚      â€¢ CSV no GCS                                           â”‚
â”‚      â€¢ Tabela externa BigQuery                              â”‚
â”‚      â†“                                                      â”‚
â”‚  ğŸ¥ˆ SILVER (Cleaned Data)                                   â”‚
â”‚      â€¢ ValidaÃ§Ã£o de coordenadas                             â”‚
â”‚      â€¢ RemoÃ§Ã£o de duplicatas                                â”‚
â”‚      â€¢ Campos derivados                                     â”‚
â”‚      â€¢ Views no BigQuery                                    â”‚
â”‚      â†“                                                      â”‚
â”‚  ğŸ¥‡ GOLD (Business Metrics)                                 â”‚
â”‚      â€¢ MÃ©tricas agregadas                                   â”‚
â”‚      â€¢ KPIs por linha e perÃ­odo                             â”‚
â”‚      â€¢ Tabelas particionadas                                â”‚
â”‚      â€¢ Pronto para dashboards                               â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ—ï¸ Arquitetura

```mermaid
flowchart TB
    A[API BRT] -->|Captura minuto a minuto| B[Prefect Flow]
    B -->|Agrega 10 min| C[CSV Local]
    C -->|Upload| D[Google Cloud Storage]
    D -->|Tabela Externa| E[BigQuery - Bronze]
    E -->|DBT Transform| F[BigQuery - Silver]
    F -->|DBT Aggregate| G[BigQuery - Gold]
    G -->|Dashboards| H[Data Visualization]
    
    I[Prefect Server] -.->|Orquestra| B
    J[Docker Agent] -.->|Executa| B
```

## ğŸ› ï¸ Tecnologias

- **OrquestraÃ§Ã£o**: Prefect 1.4.1
- **TransformaÃ§Ã£o**: DBT (Data Build Tool) 1.5.0
- **Cloud**: Google Cloud Platform (GCS + BigQuery)
- **ContainerizaÃ§Ã£o**: Docker & Docker Compose
- **Linguagem**: Python 3.8+
- **Logs**: Loguru

## ğŸ“¦ PrÃ©-requisitos

### Software NecessÃ¡rio

- Python 3.8 ou superior
- Docker & Docker Compose
- Git
- Conta Google Cloud Platform (nÃ­vel gratuito disponÃ­vel)

### Conta GCP

1. Criar projeto no [Google Cloud Console](https://console.cloud.google.com/)
2. Ativar APIs:
   - BigQuery API
   - Cloud Storage API
3. Criar Service Account com permissÃµes:
   - BigQuery Admin
   - Storage Admin
4. Baixar arquivo JSON de credenciais

## ğŸš€ InstalaÃ§Ã£o

### 1. Clone o RepositÃ³rio

```bash
git clone https://github.com/seu-usuario/desafio-civitas-brt.git
cd desafio-civitas-brt
```

### 2. Crie Ambiente Virtual Python

```bash
# Windows (PowerShell)
python -m venv venv
.\venv\Scripts\Activate.ps1

# Linux/Mac
python3 -m venv venv
source venv/bin/activate
```

### 3. Instale DependÃªncias

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### 4. Configure Credenciais GCP

Copie o arquivo de credenciais JSON para o diretÃ³rio `config/`:

```bash
# Windows
New-Item -ItemType Directory -Force -Path config
Copy-Item caminho\para\suas-credenciais.json config\gcp-credentials.json

# Linux/Mac
mkdir -p config
cp /caminho/para/suas-credenciais.json config/gcp-credentials.json
```

## âš™ï¸ ConfiguraÃ§Ã£o

### 1. VariÃ¡veis de Ambiente

Copie o arquivo de exemplo e configure:

```bash
# Windows
Copy-Item .env.example .env

# Linux/Mac
cp .env.example .env
```

Edite o arquivo `.env`:

```bash
# Google Cloud Platform
GCP_PROJECT_ID=seu-projeto-gcp
GCS_BUCKET_NAME=brt-data-civitas
GCP_CREDENTIALS_PATH=./config/gcp-credentials.json

# BigQuery
BQ_DATASET=brt_dataset

# API BRT
BRT_API_URL=https://jeap.rio.rj.gov.br/je-api/api/v2/gps

# ConfiguraÃ§Ãµes do Pipeline
CAPTURE_INTERVAL_SECONDS=60
AGGREGATION_MINUTES=10

# Prefect
PREFECT_SERVER_HOST=0.0.0.0
PREFECT_SERVER_PORT=4200
```

### 2. Criar Bucket GCS

```bash
# Via gcloud CLI
gcloud storage buckets create gs://brt-data-civitas --location=US

# OU via console: https://console.cloud.google.com/storage
```

### 3. Criar Dataset BigQuery

```bash
# Via bq CLI
bq mk --location=US brt_dataset

# OU via console: https://console.cloud.google.com/bigquery
```

### 4. Instalar DependÃªncias DBT

```bash
cd dbt_brt
dbt deps
cd ..
```

## ğŸ¬ ExecuÃ§Ã£o

### OpÃ§Ã£o 1: Prefect Server + Docker Agent (Recomendado)

#### 1. Inicie os ServiÃ§os Docker

```bash
docker-compose up -d
```

Aguarde alguns segundos para os serviÃ§os iniciarem.

#### 2. Acesse o Prefect UI

Abra no navegador: [http://localhost:4200](http://localhost:4200)

#### 3. Registre o Flow

```bash
python pipeline/brt_flow.py register
```

#### 4. Execute o Flow via UI

No Prefect UI:
- Navegue atÃ© "Flows"
- Selecione "BRT Data Pipeline - Medallion Architecture"
- Clique em "Quick Run"

### OpÃ§Ã£o 2: ExecuÃ§Ã£o Local (Desenvolvimento)

```bash
# Execute o flow diretamente
python pipeline/brt_flow.py
```

### Teste Individual de Componentes

#### Teste Captura da API

```bash
python scripts/brt_api_capture.py
```

#### Teste AgregaÃ§Ã£o

```bash
python scripts/brt_data_aggregator.py
```

#### Teste Upload GCS

```bash
python scripts/gcs_manager.py
```

#### Teste DBT

```bash
cd dbt_brt

# Cria tabela externa
dbt run-operation stage_external_sources

# Executa transformaÃ§Ãµes
dbt run

# Executa testes
dbt test

# Gera documentaÃ§Ã£o
dbt docs generate
dbt docs serve
```

## ğŸ“ Estrutura do Projeto

```
desafio-civitas-brt/
â”œâ”€â”€ ğŸ“„ README.md                    # Este arquivo
â”œâ”€â”€ ğŸ“„ requirements.txt             # DependÃªncias Python
â”œâ”€â”€ ğŸ“„ docker-compose.yml           # ConfiguraÃ§Ã£o Docker
â”œâ”€â”€ ğŸ“„ .env.example                 # Exemplo de variÃ¡veis de ambiente
â”œâ”€â”€ ğŸ“„ .gitignore
â”‚
â”œâ”€â”€ ğŸ“‚ pipeline/                    # Fluxos Prefect
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ brt_flow.py                 # Flow principal
â”‚
â”œâ”€â”€ ğŸ“‚ scripts/                     # Scripts Python
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ brt_api_capture.py          # Captura da API
â”‚   â”œâ”€â”€ brt_data_aggregator.py      # AgregaÃ§Ã£o de dados
â”‚   â””â”€â”€ gcs_manager.py              # Gerenciamento GCS
â”‚
â”œâ”€â”€ ğŸ“‚ dbt_brt/                     # Projeto DBT
â”‚   â”œâ”€â”€ dbt_project.yml             # ConfiguraÃ§Ã£o DBT
â”‚   â”œâ”€â”€ profiles.yml                # Perfis de conexÃ£o
â”‚   â”œâ”€â”€ packages.yml                # DependÃªncias DBT
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ models/                  # Modelos DBT
â”‚       â”œâ”€â”€ ğŸ“‚ bronze/              # Camada Bronze
â”‚       â”‚   â””â”€â”€ sources.yml         # Tabelas externas
â”‚       â”‚
â”‚       â”œâ”€â”€ ğŸ“‚ silver/              # Camada Silver
â”‚       â”‚   â”œâ”€â”€ stg_brt_gps_cleaned.sql
â”‚       â”‚   â””â”€â”€ schema.yml
â”‚       â”‚
â”‚       â””â”€â”€ ğŸ“‚ gold/                # Camada Gold
â”‚           â”œâ”€â”€ fct_brt_line_metrics.sql
â”‚           â””â”€â”€ schema.yml
â”‚
â”œâ”€â”€ ğŸ“‚ config/                      # ConfiguraÃ§Ãµes
â”‚   â””â”€â”€ gcp-credentials.json        # Credenciais GCP (nÃ£o versionado)
â”‚
â”œâ”€â”€ ğŸ“‚ data/                        # Dados locais
â”‚   â”œâ”€â”€ bronze/                     # Dados brutos
â”‚   â””â”€â”€ silver/                     # Dados processados
â”‚
â””â”€â”€ ğŸ“‚ docs/                        # DocumentaÃ§Ã£o adicional
```

## ğŸ“Š Modelos DBT

### Camada Bronze

**`brt_gps_raw`** (Tabela Externa)
- Dados brutos da API BRT
- Armazenados em CSV no GCS
- Schema fixo com 8 colunas

### Camada Silver

**`stg_brt_gps_cleaned`** (View)
- âœ… ValidaÃ§Ã£o de coordenadas GPS
- âœ… RemoÃ§Ã£o de duplicatas
- âœ… Campos derivados (data, hora, dia da semana)
- âœ… CategorizaÃ§Ã£o de velocidade
- âœ… IdentificaÃ§Ã£o de perÃ­odo do dia

### Camada Gold

**`fct_brt_line_metrics`** (Tabela Particionada)
- ğŸ“ˆ MÃ©tricas agregadas por linha e perÃ­odo
- ğŸ“Š KPIs operacionais
- ğŸ¯ Otimizada para dashboards
- ğŸ“… Particionada por data
- ğŸ” Clusterizada por linha e perÃ­odo

## ğŸ§ª Testes de Qualidade

O projeto implementa testes DBT em mÃºltiplos nÃ­veis:

### Testes de Schema

```yaml
# Coluna nÃ£o pode ser nula
- not_null

# Coluna deve ser Ãºnica
- unique

# Valores aceitos
- accepted_values:
    values: ['ManhÃ£', 'Tarde', 'Noite', 'Madrugada']
```

### Testes de Relacionamento

```yaml
# CombinaÃ§Ã£o de colunas Ãºnica
- dbt_utils.unique_combination_of_columns:
    combination_of_columns:
      - date_partition
      - line
      - period_of_day
```

### Testes de Valores

```yaml
# Valores dentro de um intervalo
- dbt_expectations.expect_column_values_to_be_between:
    min_value: -23.0
    max_value: -22.7
```

### Executar Testes

```bash
cd dbt_brt
dbt test
```

## ğŸ“ˆ Monitoramento

### Prefect UI

Acesse: [http://localhost:4200](http://localhost:4200)

- âœ… Status de execuÃ§Ã£o dos flows
- ğŸ“Š HistÃ³rico de runs
- â±ï¸ DuraÃ§Ã£o das tasks
- âŒ Logs de erros

### DBT Docs

```bash
cd dbt_brt
dbt docs generate
dbt docs serve
```

Acesse: [http://localhost:8080](http://localhost:8080)

- ğŸ“š DocumentaÃ§Ã£o de modelos
- ğŸŒ³ Lineage de dados
- ğŸ“‹ Schema das tabelas
- âœ… Resultados dos testes

### Logs do Pipeline

Logs detalhados sÃ£o salvos automaticamente com Loguru:

```bash
# Ver logs em tempo real
tail -f logs/brt_pipeline.log
```

## ğŸ› Troubleshooting

### Problema: Erro de autenticaÃ§Ã£o GCP

**SoluÃ§Ã£o:**
```bash
# Verifique se o arquivo de credenciais existe
ls config/gcp-credentials.json

# Defina a variÃ¡vel de ambiente
export GOOGLE_APPLICATION_CREDENTIALS=./config/gcp-credentials.json
```

### Problema: Prefect Server nÃ£o inicia

**SoluÃ§Ã£o:**
```bash
# Verifique os logs
docker-compose logs prefect-server

# Reinicie os containers
docker-compose restart
```

### Problema: DBT nÃ£o encontra tabelas

**SoluÃ§Ã£o:**
```bash
# Verifique se a tabela externa foi criada
cd dbt_brt
dbt run-operation stage_external_sources

# Verifique no BigQuery se o dataset existe
bq ls
```

### Problema: API BRT nÃ£o responde

**SoluÃ§Ã£o:**
- Verifique se a URL estÃ¡ correta
- Teste manualmente: https://jeap.rio.rj.gov.br/je-api/api/v2/gps
- Aguarde alguns minutos (API pode ter rate limiting)

## ğŸ“ Commits Convencionais

Este projeto segue o padrÃ£o de [Conventional Commits](https://www.conventionalcommits.org/):

```bash
# Exemplos
git commit -m "feat: adiciona captura de dados da API BRT"
git commit -m "fix: corrige validaÃ§Ã£o de coordenadas GPS"
git commit -m "docs: atualiza instruÃ§Ãµes de instalaÃ§Ã£o"
git commit -m "test: adiciona testes de qualidade DBT"
git commit -m "refactor: melhora estrutura do agregador"
```

## ğŸ¤ Contribuindo

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feat/nova-feature`)
3. Commit suas mudanÃ§as (`git commit -m 'feat: adiciona nova feature'`)
4. Push para a branch (`git push origin feat/nova-feature`)
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto foi desenvolvido como parte do desafio tÃ©cnico para a vaga de Engenheiro de Dados na CIVITAS.

## ğŸ“§ Contato

**Desenvolvido por:** [Seu Nome]
**Email:** seu.email@exemplo.com
**LinkedIn:** [Seu LinkedIn]

---

## ğŸ”— Links Ãšteis

- [Prefect v1 Docs](https://docs-v1.prefect.io/)
- [DBT Docs](https://docs.getdbt.com/)
- [BigQuery Docs](https://cloud.google.com/bigquery/docs)
- [API BRT Rio](https://jeap.rio.rj.gov.br/je-api/api/v2/gps)
- [GCP Free Tier](https://cloud.google.com/free)
- [Commits Convencionais](https://www.conventionalcommits.org/)

---

**Feito com â¤ï¸ para CIVITAS**
